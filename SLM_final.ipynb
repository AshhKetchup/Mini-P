{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQxgyT6YJV+Ak4RIHUVqeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshhKetchup/Mini-P/blob/main/SLM_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds={}"
      ],
      "metadata": {
        "id": "LUA7OIJ5q3NJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZg7hliGhRWW",
        "outputId": "af1d5d18-cc3f-4fef-a133-a965246fd235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE extinguish fire with\n",
            "DONE pound carpet with\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# Setup model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Ensure the model is on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def run_chain_of_thought(selected_task):\n",
        "    problem_statement = \"I am a highly intelligent question answering bot and I answer questions from a human perspective.\"\n",
        "\n",
        "    # Step 1: Get objects prompt\n",
        "    objects_prompt = \"\"\"Q: Which common objects in daily life can be used as a tool for humans to {}?\n",
        "    Please list 20 most suitable objects. Objects should be different from each other.\"\"\"\n",
        "\n",
        "    # Step 2: Get rationales prompt\n",
        "    rationales_prompt = \"\"\"For each object listed above, please explain the rationales for why they afford the task of {}\n",
        "    from the perspective of visual features.\"\"\"\n",
        "\n",
        "    # Step 3: Get visual features prompt\n",
        "    features_prompt = \"\"\"For each object and its rationales, please summarize the corresponding visual features in one sentence,\n",
        "    with comma-separated values of features, where each feature is described briefly.\n",
        "\n",
        "    For example:\n",
        "    1. [Object name]:\n",
        "    Rationales: [Rationale from previous response]\n",
        "    Visual Features: [feature 1], [feature 2], [feature 3], ...\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Step 1: Get objects\n",
        "    messages_step1 = [\n",
        "        {\"role\": \"system\", \"content\": problem_statement},\n",
        "        {\"role\": \"user\", \"content\": objects_prompt.format(selected_task)}\n",
        "    ]\n",
        "    input_text_step1 = tokenizer.apply_chat_template(messages_step1, tokenize=False)\n",
        "    # print(\"STEP 1: Get objects\\n\" )\n",
        "\n",
        "\n",
        "    inputs_step1 = tokenizer.encode(input_text_step1, return_tensors=\"pt\").to(device)\n",
        "    outputs_step1 = model.generate(inputs_step1, max_new_tokens=1000, temperature=0.2, top_p=0.9, do_sample=True)\n",
        "    response_step1 = tokenizer.decode(outputs_step1[0])\n",
        "    # print(\"Response for Step 1:\\n\" + response_step1)\n",
        "\n",
        "\n",
        "    # Step 2: Get rationales\n",
        "    messages_step2 = [\n",
        "        {\"role\": \"system\", \"content\": problem_statement},\n",
        "        {\"role\": \"user\", \"content\": objects_prompt.format(selected_task)},\n",
        "        {\"role\": \"assistant\", \"content\": response_step1},\n",
        "        {\"role\": \"user\", \"content\": rationales_prompt.format(selected_task)}\n",
        "    ]\n",
        "    input_text_step2 = tokenizer.apply_chat_template(messages_step2, tokenize=False)\n",
        "    # print(\"\\nSTEP 2: Get rationales\\n\")\n",
        "\n",
        "    inputs_step2 = tokenizer.encode(input_text_step2, return_tensors=\"pt\").to(device)\n",
        "    outputs_step2 = model.generate(inputs_step2, max_new_tokens=1500, temperature=0.2, top_p=0.9, do_sample=True)\n",
        "    response_step2 = tokenizer.decode(outputs_step2[0])\n",
        "    # print(\"Response for Step 2:\\n\" + response_step2)\n",
        "\n",
        "    # Step 3: Get visual features\n",
        "    messages_step3 = [\n",
        "        {\"role\": \"system\", \"content\": problem_statement},\n",
        "        {\"role\": \"user\", \"content\": objects_prompt.format(selected_task)},\n",
        "        {\"role\": \"assistant\", \"content\": response_step1},\n",
        "        {\"role\": \"user\", \"content\": rationales_prompt.format(selected_task)},\n",
        "        {\"role\": \"assistant\", \"content\": response_step2},\n",
        "        {\"role\": \"user\", \"content\": features_prompt}\n",
        "    ]\n",
        "    input_text_step3 = tokenizer.apply_chat_template(messages_step3, tokenize=False)\n",
        "    # print(\"\\nSTEP 3: Get visual features\\n\")\n",
        "\n",
        "    inputs_step3 = tokenizer.encode(input_text_step3, return_tensors=\"pt\").to(device)\n",
        "    outputs_step3 = model.generate(inputs_step3, max_new_tokens=2000, temperature=0.2, top_p=0.9, do_sample=True)\n",
        "    response_step3 = tokenizer.decode(outputs_step3[0])\n",
        "    # print(\"Response for Step 3:\\n\" + response_step3)\n",
        "\n",
        "    return response_step3\n",
        "\n",
        "# List of tasks\n",
        "tasks = [\n",
        "    \"step on\", \"sit comfortably on\", \"place flowers in\",\n",
        "    \"get potatoes out of fire with\", \"water plant with\",\n",
        "    \"get lemon out of tea with\", \"dig hole with\",\n",
        "    \"open bottle of beer with\", \"open parcel with\",\n",
        "    \"serve wine with\", \"pour sugar with\",\n",
        "    \"smear butter with\", \"extinguish fire with\",\n",
        "    \"pound carpet with\"\n",
        "]\n",
        "\n",
        "# run_chain_of_thought(tasks[0]) works finally bitchhhh\n",
        "\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "r_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model1 = RobertaModel.from_pretrained('roberta-base')\n",
        "model1.to(device)\n",
        "def get_padded_embeddings(text, max_tokens=42):\n",
        "    # Tokenize with device awareness\n",
        "    inputs = r_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_tokens,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    ).to(device)  # Move inputs to correct device\n",
        "\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model1(**inputs)\n",
        "\n",
        "    # Get embeddings\n",
        "    word_embeddings = outputs.last_hidden_state.squeeze()\n",
        "    sentence_embedding = word_embeddings.mean(dim=0)\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'input_ids': inputs['input_ids'].squeeze().cpu().tolist(),  # Move to CPU for storage\n",
        "        'attention_mask': inputs['attention_mask'].squeeze().cpu().tolist(),\n",
        "        'word_embeddings': word_embeddings.cpu(),  # Move to CPU\n",
        "        'sentence_embedding': sentence_embedding.cpu(),  # Move to CPU\n",
        "        'total_tokens': max_tokens\n",
        "    }\n",
        "\n",
        "import re\n",
        "for task in tasks[12:]:\n",
        "  response_step3 = run_chain_of_thought(task)\n",
        "  features = re.findall(r\"Visual Features: (.+)\", response_step3)\n",
        "  features = features[1:6]\n",
        "  embedding_results = [get_padded_embeddings(elem) for elem in features]\n",
        "\n",
        "  # Create word embeddings tensor with requires_grad=True\n",
        "  word_emb = [result[\"word_embeddings\"] for result in embedding_results]\n",
        "  word_emb_tensor = torch.stack(word_emb)\n",
        "  word_emb_tensor.requires_grad = True\n",
        "\n",
        "  # Create sentence embeddings tensor with requires_grad=True\n",
        "  sentence_emb = [result[\"sentence_embedding\"] for result in embedding_results]\n",
        "  sentence_emb_tensor = torch.stack(sentence_emb)\n",
        "  sentence_emb_tensor.requires_grad = True\n",
        "\n",
        "  # Store in ds dictionary\n",
        "  ds[task] = (word_emb_tensor, sentence_emb_tensor)\n",
        "\n",
        "  print(f\"DONE {task}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('final.pkl', 'wb') as handle:\n",
        "    pickle.dump(ds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "-ObEk60V26Sc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "eYajuoNP5s04",
        "outputId": "7d995db2-4e14-4d7f-9571-18339ca8c91b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\x16'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-012a26a53b16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x16'."
          ]
        }
      ]
    }
  ]
}